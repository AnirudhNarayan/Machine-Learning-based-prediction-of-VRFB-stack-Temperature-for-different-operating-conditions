# -*- coding: utf-8 -*-
"""EV_final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1A1KDD6uUor7lwsldw8dnhy_YbcSnfHAS
"""

import pandas as pd
data=pd.read_csv('5b_45A.csv')
data.head()
X=data.iloc[:,0]
y=data.iloc[:,1]
# print(y.tail())
# print(X.tail())
#print(X.head())
print(y.to_numpy().mean())

import pandas as pd;
import matplotlib.pyplot as plt
from numpy import array
from numpy import hstack
from sklearn.preprocessing import MinMaxScaler
import pandas as pd
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import r2_score

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.35,random_state=1)
# print(X_train.shape)
# print(X_test.shape)
# print(y_train.shape)
# print(y_test.shape)
# print(type(X_train))

X_train=X_train.to_numpy()
X_test=X_test.to_numpy()
y_train=y_train.to_numpy()
y_test=y_test.to_numpy()
X_data=X.to_numpy()
y_data=y.to_numpy()
from sklearn.preprocessing import StandardScaler
scaler=StandardScaler(with_mean=True,with_std=True)
X_train=scaler.fit_transform(X_train.reshape(-1,1))
X_test=scaler.transform(X_test.reshape(-1,1))

print(X_data)

import xgboost as xg

xgb_r = xg.XGBRegressor()
xgb_r.fit(X_train, y_train)
print("The score is:",xgb_r.score(X_test.reshape(-1,1),y_test.reshape(-1,1)))

# Average function:

def average(L):
  sum=0
  for i in L:
    sum+=i
  avg=sum/len(L)
  return avg

# GRAPH PLOTTING

y_pred=xgb_r.predict(X_test.reshape(-1,1))
arr=[]
for i in X_test.reshape(-1,1):
    arr.append(i)
arr.sort()

predicted =[]
for i in y_pred:
    predicted.append(i)
predicted.sort()

test= []
for i in y_test:
    test.append(i)
test.sort()

print(len(predicted))
print(len(test))

import matplotlib.pyplot as plt
import numpy as np
plt.ylabel("Temperature ")
plt.xlabel("Time ")

plt.plot(arr, test, color='g',linestyle="-",markersize=10, label='Actual Values(reference)')
plt.plot(arr, predicted, marker='o', color='b',markersize=4, linestyle="None", label='Predicated Values')
plt.legend(loc ="lower right")
plt.show()


#Only the input values the graph - for all 40 , 45 , 50, 60 site from ankur sirs work
#Combine 4 for charging = 1 graph
#Combine 4 for discharging = 1 graph

#Only for 5b 40
plt.ylabel("Predicted ")
plt.xlabel("Actual ")

plt.scatter(test,predicted,s=10,color='green',label='Predicted Values')
plt.axline([15, 15], [33, 33],label='Actual Values',color='red')
plt.legend(loc ="lower right")
plt.show()
#path = "5b_40.eps"
#plt.savefig(path, dpi=100,bbox_inches="tight")

print("The R2_score is:",r2_score(y_pred,y_test))

from sklearn.metrics import mean_squared_error
print("The RMSE is :",mean_squared_error(y_pred,y_test)**0.5)

from sklearn.metrics import mean_absolute_error
print("The MAE is :",mean_absolute_error(y_pred,y_test))

!pip install matplotlib --upgrade

"""**AVERAGING VALUES**"""

# import numpy as np
# import pandas as pd

# # Define the average function
# def average(L):
#     return np.mean(L)

# # Load the data into a pandas DataFrame
# df = pd.DataFrame({'original': y_data.reshape(-1,1).flatten(),
#                    'predicted': xgb_r.predict(X_data.reshape(-1,1)).flatten()})

# # Calculate the average of the original and predicted columns
# original_avg = average(df['original'])
# predicted_avg = average(df['predicted'])

# # Calculate the error
# error = (original_avg - predicted_avg) / (original_avg)

# # Print the results
# print("The average of input data is:", original_avg)
# print("The average of output data is:", predicted_avg)
# print("Hence the error is:", error)

xgb_r = xg.XGBRegressor()
xgb_r.fit(X_train, y_train)
y_pred=xgb_r.predict(X_test.reshape(-1,1))
print(len(X_data))
arr=[]
for i in y_test.reshape(-1,1):
    arr.append(i)
arr.sort()

predicted =[]
for i in y_pred:
    predicted.append(i)
predicted.sort()
a=average(arr)
p=average(predicted)
e=(a-p)/a
print("The average of input",average(arr))
print("The average of predicted values are",average(predicted))

print("The error is :",e)

# import xgboost as xgb
# from sklearn.model_selection import RandomizedSearchCV
# from sklearn.metrics import make_scorer, r2_score

# # Define the hyperparameters to be tuned
# params = {
#     'learning_rate': [0.05, 0.1, 0.15, 0.2],
#     'max_depth': [3, 4, 5, 6, 7, 8],
#     'min_child_weight': [1, 3, 5, 7],
#     'gamma': [0.0, 0.1, 0.2, 0.3, 0.4],
#     'colsample_bytree': [0.3, 0.4, 0.5, 0.7],
#     'subsample': [0.6, 0.7, 0.8, 0.9],
#     'reg_alpha': [0, 0.1, 0.5, 1],
#     'reg_lambda': [0.1, 1, 5, 10, 50]
# }

# # Create an XGBoost model
# #xgb_r.predict(X_test.reshape(-1,1))
# xgb_model = xgb.XGBRegressor(objective='reg:squarederror')
# r2_scorer = make_scorer(r2_score)
# # Create a RandomizedSearchCV object
# random_search = RandomizedSearchCV(
#     estimator=xgb_model,
#     param_distributions=params,
#     n_iter=100,
#     scoring='mean_squared_error',
#     cv=5,
#     verbose=3,
#     n_jobs=-1,
#     random_state=42
# )

# # Fit the RandomizedSearchCV object to the data
# random_search.fit(X_train.reshape(-1,1), y_train.reshape(-1,1))

# # Print the best parameters and the corresponding score
# print("Best parameters:", random_search.best_params_)
# print("Best score:", -random_search.best_score_)
# print("Best R2 score:", random_search.best_score_)